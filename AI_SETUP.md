# Luxaar AI Setup Guide

## ğŸ§  Overview

Luxaar AI is a **privacy-first, fully local AI tutor** powered by [Ollama](https://ollama.com). It runs entirely on your machine â€” no paid APIs, no data leaving your server.

### Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Luxaar Frontend â”‚ â”€â”€â–º â”‚  Express Proxy   â”‚ â”€â”€â–º â”‚   Ollama    â”‚
â”‚  (React/Next.js) â”‚     â”‚  (Port 3001)     â”‚     â”‚  (Port 11434)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Firestoreâ”‚ (Chat memory)
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Prerequisites

- **Node.js** 18+ 
- **npm** or **yarn**
- **4GB+ RAM** (8GB recommended for llama3)
- **GPU** optional but improves speed significantly

---

## ğŸš€ Quick Start

### Step 1: Install Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download from [ollama.com](https://ollama.com/download/windows)

### Step 2: Pull a Model

```bash
# Recommended (good balance of quality & speed)
ollama pull llama3

# Alternative options:
ollama pull mistral        # Fast, good for simple tasks
ollama pull llama3.1       # Latest LLaMA
ollama pull mixtral         # Mixture of experts, slower but powerful
ollama pull phi3            # Small & fast (Microsoft)
ollama pull gemma2          # Google's model
```

### Step 3: Start Ollama

```bash
ollama serve
```

Verify it's running:
```bash
curl http://localhost:11434/api/tags
```

### Step 4: Start the AI Proxy Server

```bash
cd server
npm install
npm start
```

You should see:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸ§  Luxaar AI Proxy Server             â•‘
â•‘   Port: 3001                             â•‘
â•‘   Ollama: http://localhost:11434         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Ollama connected! Available models: llama3
```

### Step 5: Start Luxaar Frontend

```bash
# In the root luxaar directory
npm run dev
```

The AI chatbot button (purple, bottom-right) will appear on the learning page!

---

## ğŸ¯ AI Modes

| Mode | Description | Best For |
|------|-------------|----------|
| ğŸ“ Tutor | Simple, friendly explanations | Understanding concepts |
| ğŸ“ Notes | Bullet-point summaries | Quick revision |
| ğŸ”¬ Deep Dive | Detailed, thorough analysis | In-depth study |
| ğŸ“‹ Exam Prep | Concise, exam-ready answers | Test preparation |
| ğŸ’» Code Helper | Debug and explain code | Programming courses |

---

## âš™ï¸ Configuration

### Environment Variables

Create a `.env.local` file in the root:
```env
NEXT_PUBLIC_AI_SERVER_URL=http://localhost:3001
```

### Proxy Server Environment

```bash
# Optional - set in server/.env or as env vars
AI_PORT=3001                          # Proxy server port
OLLAMA_URL=http://localhost:11434     # Ollama API URL
```

### Admin Settings

Navigate to **Dashboard â†’ AI Settings** to:
- Enable/disable AI globally
- Choose default model
- Adjust temperature (creativity)
- Set max response tokens
- View usage statistics
- Clear all chat history

---

## ğŸ”’ Security

1. **Ollama port is never exposed** â€” all requests go through the Express proxy
2. **CORS whitelisting** â€” only allowed frontend origins can make requests
3. **Auth-gated** â€” chatbot only renders for logged-in users
4. **Context restriction** â€” AI is instructed to only answer from course content
5. **No data leaves your server** â€” everything runs locally

---

## ğŸŒ Deployment Options

### Mode 1: Local Development
```
Laptop â†’ Ollama + Proxy Server + Next.js (all on localhost)
```

### Mode 2: Self-Hosted (VPS)
```bash
# On your VPS:
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3
ollama serve &

# 2. Start proxy (use PM2 for persistence)
npm install -g pm2
cd server && pm2 start index.js --name luxaar-ai

# 3. Update CORS origins in server/index.js
# 4. Update NEXT_PUBLIC_AI_SERVER_URL to your VPS IP
```

### Mode 3: Dedicated GPU Server
```bash
# On GPU machine:
ollama serve

# On web server:
# Set OLLAMA_URL=http://<gpu-machine-ip>:11434
OLLAMA_URL=http://192.168.1.100:11434 node server/index.js
```

---

## ğŸ—„ï¸ Firestore Collections

The AI system uses these Firestore collections:

### `ai_chats`
```json
{
  "userId": "string",
  "courseId": "string", 
  "courseTitle": "string",
  "messages": [
    { "role": "user|assistant|system", "content": "string", "timestamp": "ISO" }
  ],
  "mode": "tutor|notes|deepdive|exam|code",
  "model": "llama3",
  "createdAt": "ISO",
  "updatedAt": "ISO"
}
```

### `ai_settings`
```json
{
  "enabled": true,
  "model": "llama3",
  "maxTokens": 1024,
  "temperature": 0.7,
  "enabledCourses": []
}
```

### Firestore Indexes Required
Add to `firestore.indexes.json`:
```json
{
  "collectionGroup": "ai_chats",
  "queryScope": "COLLECTION",
  "fields": [
    { "fieldPath": "userId", "order": "ASCENDING" },
    { "fieldPath": "courseId", "order": "ASCENDING" },
    { "fieldPath": "updatedAt", "order": "DESCENDING" }
  ]
}
```

---

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| "AI Temporarily Unavailable" | Start Ollama: `ollama serve` |
| Proxy server won't start | Run `cd server && npm install` |
| Slow responses | Use a smaller model like `mistral` or `phi3` |
| Out of memory | Close other apps, or use a smaller model |
| CORS errors | Add your frontend URL to `ALLOWED_ORIGINS` in server/index.js |
| No models shown | Run `ollama pull llama3` first |

---

## ğŸ“Š Performance Tips

1. **Use GPU acceleration** â€” Ollama auto-detects NVIDIA/AMD GPUs
2. **Choose the right model**:
   - Fast: `phi3`, `mistral` (~2GB RAM)
   - Balanced: `llama3` (~4GB RAM)
   - Powerful: `mixtral`, `llama3.1:70b` (~32GB+ RAM)
3. **Limit max tokens** â€” 512-1024 is usually enough
4. **Lower temperature** â€” Faster, more focused responses

---

## ğŸ“ File Structure

```
luxaar/
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ package.json          # Proxy server dependencies
â”‚   â””â”€â”€ index.js              # Express proxy server
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ ai/
â”‚   â”‚       â””â”€â”€ aiService.ts  # AI service client
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”‚   â””â”€â”€ AIChatbot.tsx # Main chatbot widget
â”‚   â”‚   â””â”€â”€ admin/
â”‚   â”‚       â””â”€â”€ AISettingsClient.tsx  # Admin settings
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ dashboard/
â”‚           â””â”€â”€ admin/
â”‚               â””â”€â”€ ai-settings/
â”‚                   â””â”€â”€ page.tsx  # Admin route
â””â”€â”€ AI_SETUP.md               # This file
```
